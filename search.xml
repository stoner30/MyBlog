<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用 Core ML 创建简单游戏]]></title>
    <url>%2F2017%2F07%2F13%2Ftranslation-corelm-game%2F</url>
    <content type="text"><![CDATA[原文链接 WWDC 2017 中，iOS 推出了很多惊艳的 API，Core ML 就是最受关注的一个（当然，ARKit 也是相当酷炫的！）。Core ML 可以使开发者在不了解神经网络或机器学习算法等相关知识的前提下，轻松地在应用中使用机器学习模型。今天，我们来展示下如何用 Core ML 轻松地构建出一款游戏。我们将创建一款简单的拾荒者游戏，玩家需要在房间中转来转去，去寻找指定的物品。在开始之前，我建议读者先参考下我们之前发布的关于 Core ML 介绍的教程。 提示：本教程需要用到 Xcode 9 beta 版，另外，你还需要一部安装了 iOS 11 beta 版的设备，用来运行和测试应用，本次的应用不会在模拟器中运行，当然，你也可以使用像 iPhone 5s 这样的老款设备，但运行速度会有点慢。Xcode 9 beta 兼容 Swift 3.2 和 4.0 版本，以下代码全部基于 Swift 4.0 的语法完成。 &nbsp; 应用概述今天我们将要创建一款简单的应用，当然，玩法也很简单。当玩家点击 Start 按钮后，屏幕上方将随机出现物品名称，玩家的任务就是去找到它们。当玩家发现了指定的物品，只需将设备对准它，iPhone 会通过机器学习算法来识别它，成功找到一个物品后，会自动显示下一个物品。每次搜寻成功后，都会增加点数。另外，当玩家无法找到指定物品时，也可以选择跳过它。 本次制作的应用，在物品识别方面，与之前介绍 Core ML 的教程有些许差异，主要的区别是，我们将采用摄像头采集实时图像，而非简单地选择一张图片。 &nbsp; 创建工程打开 Xcode 9 创建工程，选择 Single View App 模板，虽然我们创建的是一款游戏，但使用 Single View App 模板已经足够了。将工程命名为 CoreMLScavenge，当然，你也可以选择你喜欢的名称。另外，确保开发语言为 Swift。 &nbsp; 成功创建工程后，取消选择工程描述文件中的 Landscape Left 和 Landscape Right 选项，我们希望这款游戏只在垂直模式下运行。 &nbsp; 创建用户界面是时候来点有意思的了！在导航面板中找到 Main.storyboard 文件，并在 View Controller 的上方和下方添加两个 View，将它们的宽度拉伸至与 View Controller 同宽，高度固定为 85px。整个的背景用来做图像采集窗口，所以我们要在之前添加的 View 组件上添加按钮和标签。将背景颜色设置为浅灰色，这样就能防止我们误将其他组件添加到背景上了。 &nbsp; 在上方的视图中添加两个 UILabel 组件，一个居中放置，另一个靠左。居中的标签用来显示所要寻找的物品，所以，尽量将它拉宽点，这样就有足够的空间显示文字了。而左边的标签仅仅用来显示两位数的得分，所以可以适当地缩小它。 &nbsp; 在下方的视图中添加两个 UILabel 和两个 UIButton 组件，一个标签显示当前记录，另一个显示历史最高记录，两个按钮中的一个显示 “Start”，点击后将开始游戏，另一个则显示为 “Skip”，当玩家找不到指定物品时，点击该按钮可以跳过。 &nbsp; 此处，我不准备介绍 Auto Layout 的相关设置了，我建议你自己完成。可以访问这里来了解如何使用 Auto Layout。如果你实在无法完成 Auto Layout 设置，那就确保在你要运行的设备上进行界面设计工作。 创建视图UI 的工作已经完毕了，下面我们要开始编码了。选择 ViewController.swift 文件，在文件的开始部分导入所需的框架。 123import MobileCoreServicesimport Visionimport AVKit &nbsp; 接下来添加 8 个 outlet 并将它们与 UI 关联起来。 12345678@IBOutlet var scoreLabel: UILabel!@IBOutlet var highscoreLabel: UILabel!@IBOutlet var timeLabel: UILabel!@IBOutlet var objectLabel: UILabel!@IBOutlet var startButton: UIButton!@IBOutlet var skipButton: UIButton!@IBOutlet var topView: UIView!@IBOutlet var bottomView: UIView! &nbsp; 现在，你的代码看起来应该是这样的： 12345678910111213141516171819202122232425import UIKitimport MobileCoreServicesimport Visionimport AVKitclass ViewController: UIViewController &#123; @IBOutlet var scoreLabel: UILabel! @IBOutlet var highscoreLabel: UILabel! @IBOutlet var timeLabel: UILabel! @IBOutlet var objectLabel: UILabel! @IBOutlet var startButton: UIButton! @IBOutlet var skipButton: UIButton! @IBOutlet var topView: UIView! @IBOutlet var bottomView: UIView! override func viewDidLoad() &#123; super.viewDidLoad() &#125; override func didReceiveMemoryWarning() &#123; super.didReceiveMemoryWarning() &#125;&#125; &nbsp; 我们再来添加一些全局变量，把它们加在 outlet 的下方即可。 12345var cameraLayer: CALayer!var gameTimer: Timer!var timeRemaining = 60var currentScore = 0var highScore = 0 &nbsp; 让我们来一一解释下它们：第 1 行：摄像头对应的层，后面会把它添加到 View 上并充满整个屏幕。第 2 行：稍后，我们将创建一个游戏计时器，把它定义为一个全局变量，我们就可以在任何方法中使它失效（停止计时）。第 3 行：这个变量用来保存剩余时间，它的初始值为 60，即游戏能持续 1 分钟。你可以任意修改它的值，让游戏时间更长或更短。第 4 行：这个变量用来保存用户的记录，没找到一件物品后，记录的分数就会加 1。第 5 行：这个变量用来保存用户的最高记录，每当应用启动后，我们会从 UserDefaults 中获取它，并把它赋值给当前变量。 将大量的代码都放入 viewDidLoad 方法中，会使代码结构相当凌乱，为此，我们会在 viewDidLoad 方法下方添加一个 viewSetup 方法，用来处理基本的 UI 设置。 将下面的内容添加到 viewDidLoad 方法中： 1viewSetup() &nbsp; 在 viewDidLoad 方法的下方添加如下代码： 123456func viewSetup() &#123; let backgroundColor = UIColor(red: 255/255, green: 255/255, blue: 255/255, alpha: 0.8) topView.backgroundColor = backgroundColor bottomView.backgroundColor = backgroundColor scoreLabel.text = "0"&#125; &nbsp; 上述代码的作用，是将上方和下方的视图背景设置为半透明，我们可以隐约它们后面的视频采集画面，另外，我们还设置了记录的分数为 0。 现在，你的代码看起来应该是这样的： 12345678910111213141516171819202122232425262728293031323334353637383940import UIKitimport MobileCoreServicesimport Visionimport AVKitclass ViewController: UIViewController &#123; @IBOutlet var scoreLabel: UILabel! @IBOutlet var highscoreLabel: UILabel! @IBOutlet var timeLabel: UILabel! @IBOutlet var objectLabel: UILabel! @IBOutlet var startButton: UIButton! @IBOutlet var skipButton: UIButton! @IBOutlet var topView: UIView! @IBOutlet var bottomView: UIView! var cameraLayer: CALayer! var gameTimer: Timer! var timeRemaining = 60 var currentScore = 0 var highScore = 0 override func viewDidLoad() &#123; super.viewDidLoad() viewSetup() &#125; func viewSetup() &#123; let backgroundColor = UIColor(red: 255/255, green: 255/255, blue: 255/255, alpha: 0.8) topView.backgroundColor = backgroundColor bottomView.backgroundColor = backgroundColor scoreLabel.text = "0" &#125; override func didReceiveMemoryWarning() &#123; super.didReceiveMemoryWarning() &#125;&#125; &nbsp; 创建摄像头现在我们来创建实时视频采集窗口，它将占用整个背景视图。在我们添加代码前，需要向用户申请访问摄像头的权限，iOS 会完成大部分的工作，我们所要做的只是描述下使用摄像头的原因。 在导航面板中找到 Info.plist 文件并添加一行条目，其键为 Privacy – Camera Usage Description，值为描述文字。 &nbsp; 下面来添加代码，我们把设置摄像头的准备工作放到一个叫 cameraSetup 的方法中。 把这段代码加在 viewDidLoad 方法中的 viewSetup 方法下面： 1cameraSetup() &nbsp; 然后，在 viewDidLoad 方法后面添加 cameraSetup 方法的定义： 123func cameraSetup() &#123; &#125; &nbsp; 现在，我们来创建 AVCaptureSession，通过它，我们就可以进行实时视频扑捉了。把下面的方法添加到 cameraSetup 方法中： 12345let captureSession = AVCaptureSession()captureSession.sessionPreset = AVCaptureSession.Preset.photolet backCamera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back)!let input = try! AVCaptureDeviceInput(device: backCamera)captureSession.addInput(input) &nbsp; 这里究竟发生了什么？第 1 行：我们创建了一个 AVCaptureSession 类型的常量。第 2 行：为保证输出品质，我们使用预设值的选项，将其设置为照片格式，这样就能使其达到更高的分辨率。第 3 行：我们采用后置摄像头创建了一个 AVCaptureDevie，这里，我们根本没理由使用前置摄像头。第 4 行：我们使用刚刚创建的 AVCaptureDevice 创建了一个 AVCaptureDeviceInput 对象。第 5 行：我们将 backCamera 作为 captureSession 的输入。 还记得我们之前创建的 cameraLayer 参数吗？马上我们就要用到它了，把 cameraLayer 作为主视图的一个子层，并设置它的大小与主视图相同。 将下面的代码添加到我们刚刚设置 captureSession 的那部分代码后： 123456cameraLayer = AVCaptureVideoPreviewLayer(session: captureSession)view.layer.addSublayer(cameraLayer)cameraLayer.frame = view.bounds view.bringSubview(toFront: topView)view.bringSubview(toFront: bottomView) &nbsp; 第 1-3 行：初始化一个 AVCaptureVideoPreviewLayer 类型的对象，并赋值给 cameraLayer，将 captureSession 作为构造方法的参数。然后，我们将 cameraLayer 作为子层添加到主视图的层中，并设置其大小为主视图的大小。第 5-6 行：这里，我们将上方和下方的两个视图，从视图体系中上前移动，这样，cameraLayer 就不会遮盖住它们。 在我们刚刚添加的代码后面，添加如下代码，这样就完成了这个方法： 1234567let videoOutput = AVCaptureVideoDataOutput()videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "buffer delegate"))videoOutput.recommendedVideoSettings(forVideoCodecType: .jpeg, assetWriterOutputFileType: .mp4) captureSession.addOutput(videoOutput)captureSession.sessionPreset = .highcaptureSession.startRunning() &nbsp; 第 1-3 行：创建数据输出对象，并设置输出参数。此时，你会发现一些编译错误，不用担心，稍候我们就会解决它。第 5-7 行：最后，我们将数据输出对象添加到 captureSession 中，并启动它。 cameraSetup 方法现在看起来应该是这个样子： 12345678910111213141516171819202122func cameraSetup() &#123; let captureSession = AVCaptureSession() captureSession.sessionPreset = AVCaptureSession.Preset.photo let backCamera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back)! let input = try! AVCaptureDeviceInput(device: backCamera) captureSession.addInput(input) cameraLayer = AVCaptureVideoPreviewLayer(session: captureSession) view.layer.addSublayer(cameraLayer) cameraLayer.frame = view.bounds view.bringSubview(toFront: topView) view.bringSubview(toFront: bottomView) let videoOutput = AVCaptureVideoDataOutput() videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "buffer delegate")) videoOutput.recommendedVideoSettings(forVideoCodecType: .jpeg, assetWriterOutputFileType: .mp4) captureSession.addOutput(videoOutput) captureSession.sessionPreset = .high captureSession.startRunning()&#125; &nbsp; 添加 Core ML 数据模型完成了 UI 和摄像头的设置，是时候来编写使用 Core ML 进行物体识别的代码了。 在编码前，我们需要将 Core ML 数据模型添加到工程中，为了使用 Core ML，你需要一个预处理的数据模型，虽然可以为这个游戏建立自己的数据模型，但我们还是决定使用苹果开发者网站中所提供的。 访问苹果开发者网站中关于机器学习的页面，滚动到页面下方，你会发现 4 种不同的预处理数据模型可供下载。 &nbsp; 译者注：在翻译该文时，苹果官网中所提供的数据模型已增加到 5 种。 &nbsp; 在这个游戏中，我们使用 Inception v3 模型，下载完毕后，将其拖拽至工程的导航面板中，点击数据模型文件，看看它都显示了什么。 &nbsp; 提示：保证数据模型的 Target Membership 处勾选了当前工程，否则应用将无法访问该文件。 &nbsp; 你会发现，这个数据模型有一个神经网络分类器，它将尺寸为 299x299 的图片作为输入，并输出两部分内容：一个表示所识别品类的可能性的字典，和一个表示所识别品类的字符串信息。 识别物体我们的短途旅行结束了，下面我们继续回到 View Controller 中，并添加两个神奇的方法。 首先添加 predict 方法，把如下代码添加到 cameraSetup 方法下方： 123456func predict(image: CGImage) &#123; let model = try! VNCoreMLModel(for: Inceptionv3().model) let request = VNCoreMLRequest(model: model, completionHandler: results) let handler = VNSequenceRequestHandler() try! handler.perform([request], on: image)&#125; &nbsp; 不要被它的代码行数骗了，它可是一个相当给力的方法： 第 2 行：创建一个 Inception v3 类型的常量。第 3 行：创建一个 VNCoreMLRequest 类型的对象，这个对象会在请求完成时调用结果处理方法（稍候我们就会实现它，所以不要在意那些编译错误）。第 4-5 行：创建一个 VNCoreMLRequestHandler 类型的对象并调用它，而 predict 方法的输入参数 image 也将作为 perform 方法的参数被传递进去。 下面，我们来实现 results 方法，通过它，我们来处理 predict 方法的处理结果，并使程序正常运行。 将下面的代码添加到 predict 方法下方： 12345678910111213141516171819func results(request: VNRequest, error: Error?) &#123; guard let results = request.results as? [VNClassificationObservation] else &#123; print("No result found") return &#125; guard results.count != 0 else &#123; print("No result found") return &#125; let highestConfidenceResult = results.first! let identifier = highestConfidenceResult.identifier.contains(", ") ? String(describing: highestConfidenceResult.identifier.split(separator: ",").first!) : highestConfidenceResult.identifier if identifier == objectLabel.text! &#123; currentScore += 1 //nextObject() &#125;&#125; &nbsp; 这个方法会在物体识别完成后调用。首先使用两个 guard 语句保证结果的存在性，如果结果存在，则把可信度最高的结果转换成字符串，并赋值给 identifier 常量。然后，程序会比对结果字符串与目标物体标签中显示的内容，如果相同，就意味着玩家找到了正确的物体，分数也会随之增加。另外，你会发现一个被注释掉的方法 nextObject()，我们暂时还没实现它。 好了，我们已经完成了这个游戏的主要工作！是不是很简单？你的 predict 方法和 results 方法看起来应该是这样的： 12345678910111213141516171819202122232425262728func predict(image: CGImage) &#123; let model = try! VNCoreMLModel(for: Inceptionv3().model) let request = VNCoreMLRequest(model: model, completionHandler: results) let handler = VNSequenceRequestHandler() try! handler.perform([request], on: image)&#125;func results(request: VNRequest, error: Error?) &#123; guard let results = request.results as? [VNClassificationObservation] else &#123; print("No result found") return &#125; guard results.count != 0 else &#123; print("No result found") return &#125; let highestConfidenceResult = results.first! let identifier = highestConfidenceResult.identifier.contains(", ") ? String(describing: highestConfidenceResult.identifier.split(separator: ",").first!) : highestConfidenceResult.identifier if identifier == objectLabel.text! &#123; currentScore += 1 //nextObject() &#125;&#125; &nbsp; 在完成这个游戏前，我们还有一件事要做，我们需要为 ViewController 添加扩展，来调用 predict 方法。将下面的代码添加到 ViewController.swift 文件的最底部： 1234567891011121314extension ViewController: AVCaptureVideoDataOutputSampleBufferDelegate &#123; func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) &#123; guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else &#123; fatalError("pixel buffer is nil") &#125; let ciImage = CIImage(cvPixelBuffer: pixelBuffer) let context = CIContext(options: nil) guard let cgImage = context.createCGImage(ciImage, from: ciImage.extent) else &#123; fatalError("cg image") &#125; let uiImage = UIImage(cgImage: cgImage, scale: 1.0, orientation: .leftMirrored) DispatchQueue.main.sync &#123; predict(image: uiImage.cgImage!) &#125; &#125;&#125; &nbsp; 这段扩展代码使 ViewController 适配 AVCaptureVideoDataOutputSampleBufferDelegate 协议，从而处理捕捉到的视频缓冲，并根据视频内容创建图像对象。我们把图像作为参数，传递给 predict 方法。现在，你可以正常编译你的代码了。这就是我们如何通过不断采集视频，并传递给内置的机器学习数据模型，来达到物体识别功能的全过程。 为游戏预制物体数组为了让游戏能够随机显示目标物体，我们准备了一个待选物体列表。我们创建了一个列表，来保存一批待选物体的名称，随后，我们会创建一个方法来随机选择物体，并告知玩家目标物体是什么。 这是一个有点长的数组，处于代码结构性的考虑，我把它放到了一个单独的 Swift 文件中，在导航面板中点击右键，并选择 New File...。 选择 Swift 文件类型并点击下一步按钮，将其命名为 “Objects”，将如下代码片段添加到刚刚创建的文件中： 123struct Objects &#123; let objectArray = ["computer keyboard", "mouse", "iPod", "printer", "digital clock", "digital watch", "backpack", "ping-pong ball", "envelope", "water bottle", "combination lock", "lampshade", "switch", "lighter", "pillow", "spider web", "sandal", "vacuum", "wall clock", "bath towel", "wallet", "poster", "chocolate"]&#125; &nbsp; 我们创建了一个名为 objectArray 的数组对象，它包含了可随机选择的常用日用品，以供 Core ML 进行查找。当然，你可以任意增减这些选项。 在进行下一步之前，让我们先回到 ViewController.swift 文件。 保存记录我们会使用 UserDefaults 保存用户的最高记录，这就意味着，我们需要一个 setter 把最高纪录保存到 UserDefaults 中，还需要一个 getter 方法去获得最高记录的值。 将下面两个方法添加到 results 方法的下方： 12345678910111213141516func getHighScore() &#123; if let score = UserDefaults.standard.object(forKey: "highscore") &#123; highscoreLabel.text = "\(score)" highScore = score as! Int &#125; else &#123; print("No highscore, setting to 0.") highscoreLabel.text = "0" highScore = 0 setHighScore(score: 0) &#125;&#125; func setHighScore(score: Int) &#123; UserDefaults.standard.set(score, forKey: "highscore")&#125; &nbsp; getHighScore 方法使用 if let 语句检查 UserDefaults 中是否存在被保存的最高记录，就把它赋值给 highScore 变量，如果不存在，则将 highScore 变量的值设为 0。setHighScore 只是把当前的记录设置为最高记录，当玩家刷新了最高记录后，我们将会调用它。 当应用启动时，我们需要调用 getHighScore 方法，这样，在打开应用后，玩家就能看到最高记录是多少。 将 getHighScore 方法添加到 viewDidLoad 方法中 cameraSetup 方法的后面。你的 viewDidLoad 方法看起来应该是这样的： 1234567override func viewDidLoad() &#123; super.viewDidLoad() viewSetup() cameraSetup() getHighScore()&#125; &nbsp; 处理游戏的运行现在，我们来让之前完成的那些方法协同工作，这样就能使游戏运行起来了。我们会通过在物品数组里随机挑选一件物品来启动游戏，还要处理游戏何时结束。 将下面的代码添加到 setHighScore 方法下面： 1234567891011121314151617181920212223242526272829303132//1func endGame() &#123; //2 startButton.isHidden = false skipButton.isHidden = true objectLabel.text = "Game Over" //3 if currentScore &gt; highScore &#123; setHighScore(score: currentScore) highscoreLabel.text = "\(currentScore)" &#125; //4 currentScore = 0 timeRemaining = 60 &#125;//5func nextObject() &#123; //6 let allObjects = Objects().objectArray //7 let randomObjectIndex = Int(arc4random_uniform(UInt32(allObjects.count))) //8 guard allObjects[randomObjectIndex] != objectLabel.text else &#123; nextObject() return &#125; //9 objectLabel.text = allObjects[randomObjectIndex] scoreLabel.text = "\(currentScore)"&#125; &nbsp; 你会问，这些方法都做了些什么？ 当游戏时间结束时，我们会调用 endGame 方法。 首先，我们显示 start 按钮，并隐藏 skip 按钮，因为游戏在非进行状态下是不需要 skip 按钮的，另外，我们还要设置 object 标签的文本为 “Game Over”。 我们要检查下玩家的记录是不是超过了最高记录，如果超过了，就调用 setHighScore 方法。 为下一局游戏重置所有变量的值。 当玩家找到了正确的物品，或者点击了 skip 按钮，则调用 nextObject 方法，这个方法会从物品数组中随机挑选物品，并显示在 object 标签上，这样，玩家就知道该去寻找什么了。 创建 objectArray 变量。 生成一个随机数用于定位数组中的元素，该随机数的选取范围为 0 到 objectArray 数组长度之间的值。 使用 guard 语句来保证本次所选的物品与上一次的物品不同，这样就能避免玩家连续找两件同样的物品了。 将 object 标签的值设置为本次所选物品的值，另外还要将 score 标签的值设置成正确的值。 提示：现在，我们已经实现了 nextObject 方法，请记得将 results 方法中的的注释去掉，还有其他使用到 nextObject 方法的地方。 &nbsp; 下面，我们来创建两个 action 方法，并把它们与 start 和 skip 按钮关联起来。在 nextObject 方法的下面添加如下代码： 123456789101112131415161718192021222324@IBAction func startButtonTapped() &#123; //1 gameTimer = Timer.scheduledTimer(withTimeInterval: 1, repeats: true, block: &#123; (gameTimer) in //2 guard self.timeRemaining != 0 else &#123; gameTimer.invalidate() self.endGame() return &#125; self.timeRemaining -= 1 self.timerLabel.text = "\(self.timeRemaining)" &#125;) //3 startButton.isHidden = true skipButton.isHidden = false nextObject() &#125; //4 @IBAction func skipButtonTapped() &#123; nextObject()&#125; &nbsp; 我们在这都做了些什么？ 首先我们添加了一个 action，当玩家点击 start 按钮时就会调用它。我们初始化了一个计时器，并赋值给 gameTimer 功能。我们使用代码块作为计时器的事件处理逻辑，当然也可以使用 selector，但这样会使代码看起来更整洁。 我们使用 guard 语句，来确保游戏时间尚有剩余，如果游戏剩余时间为 0，则使 gameTimer 失效，并调用 endGame 方法，如果游戏时间还有剩余，则将 timeRemaining 属性的值减 1，并更新 timerLabel 显示的信息。 隐藏 start 按钮并显示 skip 按钮，调用 nextObject 方法来显示第一个目标物体。 我们为 skip 按钮添加了一个 action，它的功能仅仅是调用 nextObject 方法。 提示：请确保你已经 action 和 outlet 与 Main.storyboard 中的 UI 组件关联完毕。 &nbsp; 运行游戏终于到了运行游戏的时间了，编译并启动应用。点击 start 按钮并对准目标物体，看看你能得多少分！在定位和识别物体时，你可能要等几秒钟，另外，如果你是在老设备上运行游戏，比如 iPhone 5s、iPhone 6，由于机能限制，程序的运行速度会很慢，我发现运行在 iPhone 7 上效果就会很不错，但运行在 iPhone 5s 上就会慢得要死…… &nbsp; 如你所料，它认不出任天堂 Switch 游戏机！ 译者注：此处，作者是在搞笑，目标物体实际上是开关…… &nbsp; 总结愿你在创建游戏的过程中心情愉快，同样，也希望你对 Core ML 有了更多的了解。这个游戏并不是那么完美，有很多地方有待你去改进（比如在找寻到目标物体后播放音效），这仅仅是为了演示如何使用 Core ML 的一个小程序，你可以在它的基础上尽情发挥想象力。 你可以在GitHub下载本教程的完整代码。 更多关于 Core ML 框架的细节，可以访问关于 Core ML 的官方文档。 如你喜欢这篇教程，欢迎留言给我。]]></content>
      <categories>
        <category>译文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Core ML 初体验：构建简单的图像识别应用]]></title>
    <url>%2F2017%2F07%2F12%2Ftranslation-coreml-introduction%2F</url>
    <content type="text"><![CDATA[原文链接 在 WWDC 2017 上，苹果公司发布了很多令开发者激动不已的框架和 API。在这些框架中，Core ML 无疑是最受关注的框架之一。利用 Core ML，你可将机器学习模型整合到应用中，但并不需要了解神经网络和机器学习的相关知识。另外，你可以使用一些预处理的数据模型，只要将其转换成 Core ML 的模型即可。为了演示，我们将使用苹果开发者网站所提供的 Core ML 模型。闲话少叙，让我们开始学习 Core ML 吧。 提示：为了完成教程，你需要使用 Xcode 9 beta 版，另外还需要运行 iOS 11 beta 版的设备，来测试教程中所介绍的特性。Xcode 9 beta 兼容 Swift 3.2 和 4.0 版本，以下代码全部基于 Swift 4.0 的语法完成。 &nbsp; Core ML 简介 Core ML 可以使你将种类繁多的机器学习模型整合到应用中，除了支持30多种深度学习的模型类型外，还支持如 tree ensemble、SVM 和 generalized linear model 等标准模型。由于 Core ML 构建于 Matel 和 Accelerate 这种底层技术之上，因此，它可以充分利用 CPU、GPU 所提供的超高性能。你可以在设备上运行机器学习模型，而不用将分析设备和数据分离。 苹果官方文档 - Core ML Core ML 是一款全新的框架，整合于 iOS 11 中，首次亮相于今年的 WWDC 上。有了它，你可以将机器学习模型整合进自己的应用中，多说一句，究竟什么是机器学习呢？简言之，机器学习是一种应用，这种应用可以不通过编程的方法，来给予计算机学习的能力。而模型则是机器学习算法与被处理数据结合后的产物。 &nbsp; 作为一名应用开发者，我们最关心的事，是如何把这些模型加入应用中，来做一些真正有趣的事。幸运的是，通过 Core ML，可以轻松集成多种机器学习模型，这使得开发者们可以实现很多特性，如图像识别、自然语言处理、文本预测等。 现在，你可能会问，将这种人工智能整合到应用中会不会很困难？重点来了，Core ML 相当易于使用，在这篇教程中，你会看到我们仅用 10 行代码，就将 Core ML 整合到了应用中。 是不是很酷？让我们开始吧！ Demo 预览应用很简单，用户可以通过拍照或在相册中选择照片，然后，机器学习算法会尝试预测图中的物体是什么，预测结果有可能并不那么完美，但重点是，你可以学会如何在应用中使用 Core ML。 &nbsp; Getting Started首先，打开 Xcode 9 并创建工程，选择 Single View App 模板，开发语言选择 Swift。 &nbsp; 创建用户界面编者提示：如果你不想从零开始构建 UI，可以下载初始工程模板，并直接跳转到 Core ML 部分。 &nbsp; 首先，切换至 Main.storyboard 文件，为视图添加一些 UI 组件，选中 ViewController 后，点击 Xcode 菜单栏中的 Editor -&gt; Embed In -&gt; Navigation Controller，完成该操作后，你会发现导航栏出现在了视图的顶部，双击导航栏，将标题修改为 Core ML（或者你认为合适的内容）。 &nbsp; 接着，拖拽两个 BarButtonItem：分别放置于导航栏的两侧。选择左侧的 BarButtonItem，在 Attributes Inspector 中，修改 System Item 属性为”Camera”。然后选择右侧的 BarButtonItem，修改其名称为”Library”，这两个按钮可以让用户拍照或从相册中选择照片。 最后，你还需要添加 UILabel 和 UIImageView 这两个组件。将 UIImageView 置于视图的中间，并修改其大小为 299x299。将 UILabel 置于视图地步，将其拉伸并顶满左右边框。至此，UI 部分已经完成了。 我并没有提及如何配置视图组件的 Auto Layout 设置，我建议你来尝试完成，避免出现位置有歧义的视图组件。如果你对 Auto Layout 并不熟悉，从而无法完成这些设置，请在你要运行的设备上构建 Storyboard。 &nbsp; 实现 Camera 和照片库功能UI 部分已经设计完成，让我们转到实现部分。在这部分中，我们会实现 Camera 和 Library 按钮的功能。选择 ViewController.swift 文件，使其符合 UINavigationControllerDelegate 协议，该协议是使用 UIImagePickerController 类所必须的。 1class ViewController: UIViewController, UINavigationControllerDelegate &nbsp; 然后，为 label 和 image view 添加两个 outlet，将 UIImageView 类型的变量命名为 imageView，将 UILabel 类型的变量命名为 classifier。现在，你的代码看起来应该像下面这样： 12345678910111213141516import UIKitclass ViewController: UIViewController, UINavigationControllerDelegate &#123; @IBOutlet weak var imageView: UIImageView! @IBOutlet weak var classifer: UILabel! override func viewDidLoad() &#123; super.viewDidLoad() &#125; override func didReceiveMemoryWarning() &#123; super.didReceiveMemoryWarning() &#125;&#125; &nbsp; 接下来，为两个 BarButtonItem 创建对应的 action，来响应其点击事件。将下面的方法添加到 ViewController 类中： 123456789101112131415161718192021@IBAction func camera(_ sender: Any) &#123; if !UIImagePickerController.isSourceTypeAvailable(.camera) &#123; return &#125; let cameraPicker = UIImagePickerController() cameraPicker.delegate = self cameraPicker.sourceType = .camera cameraPicker.allowsEditing = false present(cameraPicker, animated: true)&#125;@IBAction func openLibrary(_ sender: Any) &#123; let picker = UIImagePickerController() picker.delegate = self picker.sourceType = .photoLibrary picker.allowsEditing = false present(picker, animated: true)&#125; &nbsp; 我们在这两个 action 中所做的，就是创建了一个 UIImagePickerController 类型的常量，禁止用户对照片进行编辑（无论是拍照还是从相册中选取的），然后设置该常量的 delegate 为 ViewController 对象，最后，我们把 UIImagePickerController 对象展现给用户。 我们会看到一些编译错误，因为我们尚未实现 UIImagePickerControllerDelegate 类的方法，接下来，我们通过扩展的方式，使 ViewController 对象适配代理协议。 1234567extension ViewController: UIImagePickerControllerDelegate &#123; func imagePickerControllerDidCancel(_ picker: UIImagePickerController) &#123; dismiss(animated: true, completion: nil) &#125; &#125; &nbsp; 当用户取消选择图片时，通过上面的代码进行处理。另外，这段代码也是 ViewController.swift 文件的一部分，现在，你的代码看起来应该像这样： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import UIKitclass ViewController: UIViewController, UINavigationControllerDelegate &#123; @IBOutlet weak var imageView: UIImageView! @IBOutlet weak var classifer: UILabel! override func viewDidLoad() &#123; super.viewDidLoad() &#125; override func didReceiveMemoryWarning() &#123; super.didReceiveMemoryWarning() &#125; @IBAction func camera(_ sender: Any) &#123; if !UIImagePickerController.isSourceTypeAvailable(.camera) &#123; return &#125; let cameraPicker = UIImagePickerController() cameraPicker.delegate = self cameraPicker.sourceType = .camera cameraPicker.allowsEditing = false present(cameraPicker, animated: true) &#125; @IBAction func openLibrary(_ sender: Any) &#123; let picker = UIImagePickerController() picker.delegate = self picker.sourceType = .photoLibrary picker.allowsEditing = false present(picker, animated: true) &#125;&#125;extension ViewController: UIImagePickerControllerDelegate &#123; func imagePickerControllerDidCancel(_ picker: UIImagePickerController) &#123; dismiss(animated: true, completion: nil) &#125; &#125; &nbsp; 回过头来，切换到 Main.storyboard 文件，保证你设置的 outlet 和 action 均已绑定完毕。 为了访问摄像头和照片库，还有最后一件我们必须要做的事。找到工程中的 Info.plist 文件，添加两个条目：Privacy – Camera Usage Description 和 Privacy – Photo Library Usage Description。从 iOS 10 开始，若要使用摄像头和照片库，你必须描述其使用原因。 &nbsp; 好了，就这样！马上，你就要进入到本教程的核心部分！再重复一遍，如果你不想从零开始构建 UI，可以下载出事工程模板。 整合 Core ML 数据模型现在，我们来换换档，把 Core ML 数据模型整合进应用中。之前提到过，我们需要使用预处理过的数据模型，与 Core ML 协同工作。当然，你可以构建自己的模型，但在这个 demo 中，我们使用苹果开发者网站中提供的预处理模型。 访问苹果开发者网站中的机器学习页面，滚动到页面的下方，会发现4种预处理的数据模型。 译者注：在翻译该文时，苹果官网中所提供的数据模型已增加到 5 种。 &nbsp; &nbsp; 在本教程中，我们选择 Inception v3 模型，当然，你也可以去尝试其他模型。下载完成后，将它添加到工程中，看看它都显示了什么？ &nbsp; 提示：保证数据模型的 Target Membership 处勾选了当前工程，否则应用将无法访问该文件。 &nbsp; 从上图中可以发现，当前模型为神经网络分类，另外，你还要注意的信息是 Model Evaluation Parameters，它说明了模型接受的输入参数的要求，以及输出内容的信息。当前模型接收 299x299 的图片作为输入，并返回可能性最高的分类描述，以及每种分类的可能性。 另外要注意的是 Model Class，它（Inceptionv3）是由机器学习模型所生成的，可以直接在代码中使用。如果你点击 Inceptionv3 类右侧的下一步箭头，就可以看到这个类的源代码。 &nbsp; 下面，我们来把数据模型加入到代码中。切换到 ViewController.swift 文件，在文件的最开始部分，导入 Core ML 框架。 1import CoreML &nbsp; 然后，定义一个 Inceptionv3 类型的变量，并在 viewWillAppear() 方法中进行初始化： 12345var model: Inceptionv3!override func viewWillAppear(_ animated: Bool) &#123; model = Inceptionv3()&#125; &nbsp; 我知道你在想什么。 “好吧，Sai，你为什么不早点初始化它？” “在 viewWillAppear 方法中定义它的重点是什么？” 好吧，朋友，重点就是，当你的应用尝试识别图像中的物体时，它会快很多！ 如果我们回过头来看 Inceptionv3.mlmodel，你会发现，输入仅仅是一张尺寸为 299x299 的图片，那我们如何把图片转换成这个尺寸呢？别介，这就是下面我们要做的。 转换图像把 ViewController.swift 文件的扩展部分的代码更新成如下所示，我们会实现 imagePickerController(_:didFinishPickingMediaWithInfo) 来处理所选的图片。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859extension ViewController: UIImagePickerControllerDelegate &#123; func imagePickerControllerDidCancel(_ picker: UIImagePickerController) &#123; dismiss(animated: true, completion: nil) &#125; func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [String : Any]) &#123; picker.dismiss(animated: true) classifer.text = "Analyzing Image..." guard let image = info["UIImagePickerControllerOriginalImage"] as? UIImage else &#123; return &#125; UIGraphicsBeginImageContextWithOptions(CGSize(width: 299, height: 299), true, 2.0) image.draw(in: CGRect(x: 0, y: 0, width: 299, height: 299)) let newImage = UIGraphicsGetImageFromCurrentImageContext()! UIGraphicsEndImageContext() let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue, kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] as CFDictionary var pixelBuffer: CVPixelBuffer? let status = CVPixelBufferCreate(kCFAllocatorDefault, Int(newImage.size.width), Int(newImage.size.height), kCVPixelFormatType_32ARGB, attrs, &amp;pixelBuffer) guard status == kCVReturnSuccess else &#123; return &#125; CVPixelBufferLockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0)) let pixelData = CVPixelBufferGetBaseAddress(pixelBuffer!) let rgbColorSpace = CGColorSpaceCreateDeviceRGB() let context = CGContext(data: pixelData, width: Int(newImage.size.width), height: Int(newImage.size.height), bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(pixelBuffer!), space: rgbColorSpace, bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue) context?.translateBy(x: 0, y: newImage.size.height) context?.scaleBy(x: 1.0, y: -1.0) UIGraphicsPushContext(context!) newImage.draw(in: CGRect(x: 0, y: 0, width: newImage.size.width, height: newImage.size.height)) UIGraphicsPopContext() CVPixelBufferUnlockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0)) imageView.image = newImage &#125; &#125; &nbsp; 以上代码的作用： 10-14 行：在方法中的最初几行，我们从 info 字典中获取所选择的图片（使用 UIImagePickerControllerOriginalImage 键），另外，一旦我们选择完毕，就关闭 UIImagePickerController。 16-20 行：由于数据模型仅接受尺寸为 299x299 的图片，所以我们把所选图片转换成方形，接着，我们把这个方形图片赋值给另一个常量 newImage。 22-33 行：现在，我们把 newImage 转换成 CVPixelBuffer。你可能不太熟悉 CVPixelBuffer，其实，它就是内存中一块保存像素的图像缓存区。你可以在这了解更多关于它的信息。 47-48 行：我们使用所有呈现图片的像素，并将其转化为依赖设备的 RGB 色彩空间。接着，使用所有数据创建 CGContext，当我们打算渲染（或更改）一些底层参数时，可以很轻松的调用它，也就是接下来两行代码所做的——转换和缩放图片。 50-55 行：最后，我们生成图像，移除栈顶的上下文内容，并将 imageView.image 的值设置为 newImage。 即使你不太了解上面的代码也没关系，这些都是高级的 Core Image 代码，而这并不在本教程所涵盖的范围内。你只需要知道，我们将图像转换成了数据模型可以接受的格式。我建议你修改其中的数字并观察最终的生成结果，来更好地理解上述代码。 使用 Core ML让我们将焦点移回到 Core ML。我们使用 Inceptionv3 模型来实现物体识别，来吧，只需要几行代码而已。将下面的代码片段粘贴到 imageView.image = newImage 的下一行。 12guard let prediction = try? model.prediction(image: pixelBuffer!) else &#123; return &#125;classifer.text = "I think this is a \(prediction.classLabel)." &nbsp; 搞定了！使用 Inceptionv3 类中的 prediction(image:) 方法来预测图像中的物体。我们将 pixelBuffer（重置大小的图像）作为参数传递给方法，一旦预测完成，预测结果会以字符串的形式返回，我们只要更新 classifier 的 text 属性，即可显示识别结果。 是时候来测试一下我们的应用了！编译代码并在模拟器中运行它，拍照或从照片库中选择一张照片，应用就会告诉你，它识别出了什么。 &nbsp; 在测试过程中，有时你可能会发现识别结果并不正确。这并非程序的问题，而是由于预处理数据模型造成的。 &nbsp; 总结我希望你已经了解了如何将 Core ML 整合进应用中，这只是一片介绍性的教程，如果你有兴趣将预处理的 Caffe、Keras 或 SciKit 模型转换成 Core ML 模型，请继续关注 Core ML 系列教程的下一篇，我会告诉你如何转换它们。 你可以在 GitHub 上获取本教程的全部代码。 更多关于 Core ML 框架的细节，可通过访问苹果官方网站进行查询。你也可以参考 WWDC 2017 中关于 Core ML 的相关部分： Introducing Core ML Core ML in Depth 你对 Core ML 有何感想？请留言给我！ 原文链接]]></content>
      <categories>
        <category>译文</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2017年书单]]></title>
    <url>%2F2017%2F07%2F06%2F2017%E5%B9%B4%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[未读 《宁静的森林水池》 《刀锋》 《乌克兰拖拉机简史》 《培养孩子的英文耳朵》 《大问题》 《大历史》 《佛祖都说了些什么？》 《那些古怪又让人忧心的问题：What if？》 《当尼克松遇上毛泽东》 《暗时间》 《30天学会绘画》 《影响力》 《演讲模式》 《RxSwift: Reactive Programming with Swift》 《3D Apple Games by Tutorials》 《PPT，要你好看》 已读 《一个叫欧维的男人决定去死》 《人民的名义》 《杀死一只知更鸟》 《小岛经济学》 《房思琪的初恋乐园》 《时间的形状》 《外婆的道歉信》 《人骨拼图》 备注未读清单有可能不间断更新，主要看有无好书、个人时间安排等。]]></content>
      <categories>
        <category>读书</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[归来记]]></title>
    <url>%2F2017%2F07%2F05%2F%E5%BD%92%E6%9D%A5%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[回归十几年前，曾经安家过新浪博客。 起初的动机，只是伙伴们之间发发牢骚，感慨下岁月如梭，平添几分对往昔的回忆。渐渐地，当热度慢慢消散，博客这个事物也就慢慢地淡出了我的生活。现在看来，那时的自己当真是年轻，只不过刚刚经历人生，就要去感慨它，没阅历，没资历，有的也仅仅是颗浮躁的心。 现如今，工作十年，除了日常写写文档、画画流程图，出于个人意愿的编排文字的工作基本上再没碰过。生活的美好，已被渐渐老去的心所蒙蔽，纵有令人心潮澎湃的一瞬，也是稍纵即逝。 出于对生活、工作、童心等等的怀恋，我觉得应该继续动动手，写写画画了。 于是，我回来了！ 安家再度归来的我，打算采用独立博客的解决方案，挑来选去，最终决定使用Hexo。 对于技术出身的我来说，基于Node.js开发的Hexo的安装和配置并非难事，再配以简约风格的Next主题，实乃完美！其生成的静态页面，可发布至GitHub上，与其他资源统一管理，维护成本也会相对降低！ 辗转几日，已基本修缮完毕，满心欢喜地搬进新家，有一种温馨感，因为今后，我要靠她来填满人生中消散的回忆了。 生活生活是一本书，书的作者就是我们自己。有悲有喜，五味杂陈，缺一不可。 今后，我会坚持把生活中的点滴，堆叠进这个小屋中，让她多彩起来，也让生活这本书生动起来。 Follow your heart, keep hungry and keep stupid.by Steven Jobs]]></content>
      <categories>
        <category>杂货铺</category>
      </categories>
  </entry>
</search>